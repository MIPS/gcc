/* From fold_unary in order of appearance.  */

/* If we have (type) (a CMP b) and type is an integral type, return
   new expression involving the new type.  Canonicalize
   (type) (a CMP b) to (a CMP b) ? (type) true : (type) false for
   non-integral type.
   Do not fold the result as that would not simplify further, also
   folding again results in recursions.  */
/* ???  Eh, do we want sth like (define-ops cmp lt le eq ...) to not
   repeat this too many times?  */
(for cmp (lt le eq ne ge gt unordered ordered unlt unle ungt unge uneq ltgt)
 (simplify
   (convert (cmp@2 @0 @1))
   (if (TREE_CODE (type) == BOOLEAN_TYPE)
    (cmp @0 @1))
   /* Not sure if the following makes sense for GIMPLE.  */
   (if (!INTEGRAL_TYPE_P (type) && !VOID_TYPE_P (type)
	&& TREE_CODE (type) != VECTOR_TYPE)
    (cond @2
      { constant_boolean_node (true, type); }
      { constant_boolean_node (false, type); }))))


/* Convert (T1)(~(T2)X) into ~(T1)X if T1 and T2 are integral types
   of the same precision, and X is an integer type not narrower than
   types T1 or T2, i.e. the cast (T2)X isn't an extension.  */
(simplify
 (convert (bit_not@0 (convert @1)))
 (if (INTEGRAL_TYPE_P (type)
      && INTEGRAL_TYPE_P (TREE_TYPE (@0))
      && TYPE_PRECISION (type) == TYPE_PRECISION (TREE_TYPE (@0))
      && INTEGRAL_TYPE_P (TREE_TYPE (@1))
      && TYPE_PRECISION (type) <= TYPE_PRECISION (TREE_TYPE (@1)))
  (bit_not (convert @1))))

/* Convert (T1)(X * Y) into (T1)X * (T1)Y if T1 is narrower than the
   type of X and Y (integer types only).  */
(simplify
 (convert (mult @0 @1))
 (if (INTEGRAL_TYPE_P (type)
      && INTEGRAL_TYPE_P (TREE_TYPE (@0))
      && TYPE_PRECISION (type) < TYPE_PRECISION (TREE_TYPE (@0))
      /* ???  These kind of patterns are a bad idea - see PR41043.  We
	 create a lot of redundant statements if operands are used multiple
	 times.  Maybe we want a flag for this.  But eventually these
	 kind of transforms should be done in a pass.  */
      && (GENERIC
          || TREE_CODE (@0) != SSA_NAME || TREE_CODE (@1) != SSA_NAME
	  || ((TREE_CODE (@0) != SSA_NAME || has_single_use (@0))
	       && (TREE_CODE (@1) != SSA_NAME || has_single_use (@1)))))
  (if (TYPE_OVERFLOW_WRAPS (type))
   (mult (convert @0) (convert @1)))
  (with { tree utype = unsigned_type_for (type); }
   (convert (mult (convert:utype @0) (convert:utype @1))))))


/* From tree-ssa-forwprop.c:combine_conversions.  */

/* Combine two conversions in a row.  */
(for ocvt (convert float fix_trunc)
 (for icvt (convert float)
  (simplify
   (ocvt (icvt@1 @0))
   (with
    {
      tree inside_type = TREE_TYPE (@0);
      tree inter_type = TREE_TYPE (@1);
      int inside_int = INTEGRAL_TYPE_P (inside_type);
      int inside_ptr = POINTER_TYPE_P (inside_type);
      int inside_float = FLOAT_TYPE_P (inside_type);
      int inside_vec = TREE_CODE (inside_type) == VECTOR_TYPE;
      unsigned int inside_prec = TYPE_PRECISION (inside_type);
      int inside_unsignedp = TYPE_UNSIGNED (inside_type);
      int inter_int = INTEGRAL_TYPE_P (inter_type);
      int inter_ptr = POINTER_TYPE_P (inter_type);
      int inter_float = FLOAT_TYPE_P (inter_type);
      int inter_vec = TREE_CODE (inter_type) == VECTOR_TYPE;
      unsigned int inter_prec = TYPE_PRECISION (inter_type);
      int inter_unsignedp = TYPE_UNSIGNED (inter_type);
      int final_int = INTEGRAL_TYPE_P (type);
      int final_ptr = POINTER_TYPE_P (type);
      int final_float = FLOAT_TYPE_P (type);
      int final_vec = TREE_CODE (type) == VECTOR_TYPE;
      unsigned int final_prec = TYPE_PRECISION (type);
      int final_unsignedp = TYPE_UNSIGNED (type);
    }
   /* In addition to the cases of two conversions in a row
      handled below, if we are converting something to its own
      type via an object of identical or wider precision, neither
      conversion is needed.  */
   (if (((GIMPLE && useless_type_conversion_p (type, inside_type))
	 || (GENERIC
	     && TYPE_MAIN_VARIANT (type) == TYPE_MAIN_VARIANT (inside_type)))
	&& (((inter_int || inter_ptr) && final_int)
	    || (inter_float && final_float))
	&& inter_prec >= final_prec)
    @0)

   /* Likewise, if the intermediate and initial types are either both
      float or both integer, we don't need the middle conversion if the
      former is wider than the latter and doesn't change the signedness
      (for integers).  Avoid this if the final type is a pointer since
      then we sometimes need the middle conversion.  Likewise if the
      final type has a precision not equal to the size of its mode.  */
   (if (((inter_int && inside_int)
	 || (inter_float && inside_float)
	 || (inter_vec && inside_vec))
	&& inter_prec >= inside_prec
	&& (inter_float || inter_vec
	    || inter_unsignedp == inside_unsignedp)
	&& ! (final_prec != GET_MODE_PRECISION (TYPE_MODE (type))
	      && TYPE_MODE (type) == TYPE_MODE (inter_type))
	&& ! final_ptr
	&& (! final_vec || inter_prec == inside_prec))
    (ocvt @0))

   /* If we have a sign-extension of a zero-extended value, we can
      replace that by a single zero-extension.  Likewise if the
      final conversion does not change precision we can drop the
      intermediate conversion.  */
   (if (inside_int && inter_int && final_int
	&& ((inside_prec < inter_prec && inter_prec < final_prec
	     && inside_unsignedp && !inter_unsignedp)
	    || final_prec == inter_prec))
    (convert @0))

   /* Two conversions in a row are not needed unless:
	- some conversion is floating-point (overstrict for now), or
	- some conversion is a vector (overstrict for now), or
	- the intermediate type is narrower than both initial and
	  final, or
	- the intermediate type and innermost type differ in signedness,
	  and the outermost type is wider than the intermediate, or
	- the initial type is a pointer type and the precisions of the
	  intermediate and final types differ, or
	- the final type is a pointer type and the precisions of the
	  initial and intermediate types differ.  */
   (if (! inside_float && ! inter_float && ! final_float
	&& ! inside_vec && ! inter_vec && ! final_vec
	&& (inter_prec >= inside_prec || inter_prec >= final_prec)
	&& ! (inside_int && inter_int
	      && inter_unsignedp != inside_unsignedp
	      && inter_prec < final_prec)
	&& ((inter_unsignedp && inter_prec > inside_prec)
	    == (final_unsignedp && final_prec > inter_prec))
	&& ! (inside_ptr && inter_prec != final_prec)
	&& ! (final_ptr && inside_prec != inter_prec)
	&& ! (final_prec != GET_MODE_PRECISION (TYPE_MODE (type))
	      && TYPE_MODE (type) == TYPE_MODE (inter_type)))
    (ocvt @0))

   /* A truncation to an unsigned type should be canonicalized as
      bitwise and of a mask.  */
   (if (final_int && inter_int && inside_int
	&& final_prec == inside_prec
	&& final_prec > inter_prec
	&& inter_unsignedp)
    (convert (bit_and @0 { wide_int_to_tree
	                     (inside_type,
			      wi::mask (inter_prec, false,
					TYPE_PRECISION (inside_type))); })))

   /* If we are converting an integer to a floating-point that can
      represent it exactly and back to an integer, we can skip the
      floating-point conversion.  */
   (if (inside_int && inter_float && final_int &&
	(unsigned) significand_size (TYPE_MODE (inter_type))
	>= inside_prec - !inside_unsignedp)
    (convert @0))))))

/* From tree-ssa-forwprop.c:simplify_conversion_from_bitmask.  */

/* If we have a narrowing conversion to an integral
   type that is fed by a BIT_AND_EXPR, we might be
   able to remove the BIT_AND_EXPR if it merely
   masks off bits outside the final type (and nothing
   else.  */
(simplify
  (convert (bit_and @0 INTEGER_CST@1))
  (if (INTEGRAL_TYPE_P (type)
       && INTEGRAL_TYPE_P (TREE_TYPE (@0))
       && TYPE_PRECISION (type) <= TYPE_PRECISION (TREE_TYPE (@0))
       && operand_equal_p (@1, build_low_bits_mask (TREE_TYPE (@1),
						    TYPE_PRECISION (type)), 0))
   (convert @0)))
